{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![26-Weeks-Logo](../images/26-weeks-of-data-science-banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Support Vector Machines</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Introduction \n",
    "- Working\n",
    "- SVM Implementation in Python\n",
    "- Different SVM Kernels\n",
    "- Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we going to learn today ?\n",
    "***\n",
    "- SVM : An Introduction\n",
    "\t- What is SVM\n",
    "\t- Hyperplane\n",
    "\t- Support Vectors\n",
    "- SVM in Python\n",
    "- Soft Margin SVM\n",
    "\t- What is Soft Margin ?\n",
    "\t- Regularization Parameters\n",
    "\t\t- Significance of `C`\n",
    "\t\t- Python implementation of `C`\n",
    "- SVM Kernels\n",
    "\t- What are Kernels ?\n",
    "\t- What is the kernel trick ?\n",
    "- Types of Kernels\n",
    "\t- Linear Kernel\n",
    "\t- Polynomial Kernel\n",
    "\t- Radial Basis Function kernel/ Gaussian Kernel\n",
    "\t- Effect of Gamma\n",
    "\t- Summary Table\n",
    "\t- Python implementation of Kernels\n",
    "- Working with Kernels\n",
    "- Extensions to SVM\n",
    "\t- Multiclass SVM\n",
    "\t\t- One vs All (OVA)\n",
    "\t\t- One vs One (OVO)\n",
    "\t- Python implementation of multi-class\n",
    "- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is SVM?\n",
    "\n",
    "\n",
    "A big company wants to venture into the mobile phone business. The manufacturing of new devices is done but the company has doubts regarding the pricing of phones. It wants to make a decision based on the market trend. \n",
    "\n",
    "They don't want the exact price but they do want to know whether a phone belongs to `budget phones`(price<400 dollars) or  `flagship phones`(price>400 dollars) zone.\n",
    "\n",
    "\n",
    "You take two important features of a smart phone namely `Battery_Power` and `Clock Speed` and decide to plot the binary targets against each other and get the following:\n",
    "\n",
    "![](../images/svm_graph_1.jpg)\n",
    "\n",
    "\n",
    "\n",
    "It's clear that these two features are not a good measure to differentiate between the target class.\n",
    "\n",
    "You decide to add one more feature `ram` and plot the resulting distribution:\n",
    "\n",
    "\n",
    "![](../images/svm_graph_2.jpg)\n",
    "\n",
    "\n",
    "By plotting the same data in a higher dimension, we are able to identify a decision boundary that separates the target class\n",
    "\n",
    "\n",
    "With an analogous reasoning, it can be assumed that for data in `k` dimensions there exists `z` dimension(z>k) that `linearly` separates the target class\n",
    "\n",
    "That is what the **Support Vector Machine**(SVM) classifier attempts to do.\n",
    "\n",
    "\n",
    "### What is SVM?\n",
    "\n",
    "Support Vector Machines are based on the concept of decision planes that define decision boundaries. \n",
    "In other words, given labeled training data (supervised learning), the algorithm outputs an optimal `hyperplane` which can help categorize new examples. \n",
    "\n",
    "### Hyperplane\n",
    "\n",
    "Hyperplane is defined as a subspace whose dimension is one less than that of its ambient space. \n",
    "\n",
    "For e.g: If the space is 2-D, its hyperplanes are lines(1-D) whereas if data space is 3-D then its hyperplanes are the 2-D planes.\n",
    "\n",
    "\n",
    "Going back to our phone example, one thing you might have wondered. \n",
    "While classifying the price, the challenge is not only finding a hyperplane(decision boundary) but also in finding the optimum hyperplane.\n",
    "\n",
    "\n",
    "![](../images/svm_graph_3.jpg)\n",
    "\n",
    "In the above plot, each of the three lines divides the boundary between the target class.\n",
    "How does SVM identify the **optimum** hyperplane?\n",
    "\n",
    "Let's find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vectors\n",
    "\n",
    "As discussed above, the challenge is to find the `'optimum'` `'hyperplane'`.\n",
    "\n",
    "**What does identifying the optimum hyper-plane entail?**\n",
    "\n",
    "Let’s understand it first by taking the simplest example of data in `2-Dimensional Space`.\n",
    "\n",
    "Consider the mobile price example, for two features X1 and X2, following is the distribution of the data:\n",
    "\n",
    "\n",
    "![](../images/svm_graph_4.jpg)\n",
    "\n",
    "**Case-1:**\n",
    "\n",
    "Here, we have two hyper-planes (A, B).\n",
    "\n",
    "![](../images/svm_graph_4_b.jpg)\n",
    "\n",
    "Which is the best hyper-plane?\n",
    "\n",
    "As a thumb rule remember: “Select the hyper-plane which segregates the two classes better”.\n",
    "\n",
    "In the above scenario, hyper-plane “A” satisifes the thumb rule.\n",
    "\n",
    "\n",
    "**Case-2:**\n",
    "\n",
    "Here, we have three hyper-planes (A, B and C) and all are segregating the classes well. \n",
    "\n",
    "![](../images/svm_graph_5.jpg)\n",
    "\n",
    "Which is the best hyper-plane?\n",
    "\n",
    "**Maximizing the margins** between nearest data point (of either class) and hyper-plane will result in choosing the right hyper-plane.\n",
    "\n",
    "![](../images/svm_graph_5_b.jpg)\n",
    "\n",
    "The nearest data points are known as `support vectors` \n",
    "\n",
    "**Support Vectors**\n",
    "\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. \n",
    "Using these support vectors, SVM maximizes the margin of the classifier.Reason for maximizing the margin distance is that it gives greater confidence for future data points to be classified correctly.\n",
    "\n",
    "\n",
    "Updating the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.\n",
    "\n",
    "![](../images/svm_graph_6.jpg)\n",
    "\n",
    "The margin for hyper-plane B is the highest \n",
    "\n",
    "Hence, the right hyper-plane is B. \n",
    "\n",
    "\n",
    "**Case-3:**\n",
    "\n",
    "![](../images/svm_graph_7.jpg)\n",
    "\n",
    "Which is the best hyper-plane?\n",
    "\n",
    "Hyper-plane B has higher margin compared to A and seems like the correct choice.\n",
    "\n",
    "But SVM will select Hyper-plane A.\n",
    "\n",
    "**Reason:** SVM first selects the hyper-plane which classifies the classes more accurately before maximizing margin. \n",
    "\n",
    "Because hyper-plane B has a classification error and hyper-plane A has none, SVM will select hyper-plane A.\n",
    "\n",
    "**Case-4:** \n",
    "Assume another distribution(similar to the one we encountered in `Battery_Power` v `Clock_Speed`):\n",
    "\n",
    "![](../images/svm_graph_8.jpg)\n",
    "\n",
    "\n",
    "We don't have linear hyper-plane between the two classes, so how does one classify these two classes? \n",
    "\n",
    "\n",
    "We apply **transformation** and add one more dimension(z-axis). \n",
    "\n",
    "Let's assume value of points on z plane, $w = x_{1}^2 + x_{2}^2$. \n",
    "\n",
    "Now if we plot in z-axis, a clear separation is visible and a line can be drawn\n",
    "\n",
    "![](../images/svm_graph_9.jpg)\n",
    "\n",
    "**Q:** Does SVM always add/create a feature corresponsing to Z-Dimension to create decision boundary for the above mentioned type of data points distribution?\n",
    "\n",
    "**A:** Projecting the existing points to a higher dimension is computationally expensive. To resolve that, SVM implements something called `kernel trick`(More on that later).\n",
    "\n",
    "*** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM in python\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The company has now extracted real data from the market and wants to make a decision based on the same. \n",
    "\n",
    "The data contains columns describing the features of 2000 different phone models and target variable telling us whether it's a `budget phone`(price<400 dollars) or  `flagship phone`(price>400 dollars) zone.\n",
    "\n",
    "Following are it's columns:\n",
    "\n",
    "\n",
    "`battery_power` : Total energy a battery can store in one time measured in mAh\n",
    "\n",
    "`blue` : Has bluetooth or not\n",
    "\n",
    "`clock_speed` : Speed at which microprocessor executes instructions\n",
    "\n",
    "`dual_sim` : Has dual sim support or not\n",
    "\n",
    "`fc` : Front Camera mega pixels\n",
    "\n",
    "`four_g` : Has 4G or not\n",
    "\n",
    "`int_memory`: Internal Memory in GB\n",
    "\n",
    "`m_dep` : Mobile depth in cm\n",
    "\n",
    "`mobile_wt` : Weight of mobile phone\n",
    "\n",
    "`n_cores` : Number of cores of processor\n",
    "\n",
    "`pc` : Primary Camera mega pixels\n",
    "\n",
    "`px_height` : Pixel Resolution Height\n",
    "\n",
    "`px_width` : Pixel Resolution Width\n",
    "\n",
    "`ram` : Random Access Memory in Megabytes\n",
    "\n",
    "`sc_h` : Screen Height of mobile in cm\n",
    "\n",
    "`sc_w` : Screen Width of mobile in cm\n",
    "\n",
    "`talk_time` : Longest time that a single battery charge will last when you are on a call\n",
    "\n",
    "`three_g` : Has 3G or not\n",
    "\n",
    "`touch_screen` : Has touch screen or not\n",
    "\n",
    "`wifi` : Has wifi or not\n",
    "\n",
    "`price_range[Target]` :  Whether phone's price range is less than 400 dollars(0) or more than 400 dollars(1)\n",
    "\n",
    "We need to create a model that will correctly classify the price range.\n",
    "\n",
    "We will take a subset of the above data(200 rows) to understand the implementation of SVM in python\n",
    "**SVM in Sklearn**\n",
    "\n",
    "```python\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "# Loading of data\n",
    "DF=pd.read_csv(\"../data/train_sample.csv\")\n",
    "\n",
    "# Printing of value counts\n",
    "print(DF['price_range'].value_counts())\n",
    "\n",
    "X=DF.drop('price_range',1)\n",
    "y=DF['price_range'].copy()\n",
    "\n",
    "\n",
    "# Splitting the data to train and test\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Initiating support vector object\n",
    "model= svm.LinearSVC(random_state=0)\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "#Calculating the accuracy\n",
    "accuracy=model.score(X_test,y_test)\n",
    "\n",
    "print(\"Accuracy of the model: \",accuracy)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "\n",
    "0    108\n",
    "1     92\n",
    "Name: price_range, dtype: int64\n",
    "\n",
    "Accuracy of the model:  0.8333333333333334\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Soft Margin?\n",
    "\n",
    "\n",
    "In previous discussions of SVM, we always had an underlying assumption that data is linearly separable.\n",
    "\n",
    "But we know real life data is often noisy. Even when the data is linearly separable, a lot of things can happen. One of the difficult things to handle is outlier.\n",
    "\n",
    "In the presence of an outlier, there can be two cases: \n",
    "\n",
    "* The outlier can be closer to the examples of other class than most of the examples of its class,thus reducing the margin.\n",
    "\n",
    "* Or it can be among the other examples and break linear separability. \n",
    "\n",
    "Let us see how the hard margin SVM deals with them.\n",
    "\n",
    "\n",
    "Consider the following example plot with the simple linear division:\n",
    "\n",
    "![](../images/soft_margin_1.jpg)\n",
    "\n",
    "**Case 1:**\n",
    "The same division when we add an outlier:\n",
    "\n",
    "\n",
    "![](../images/soft_margin_2.jpg)\n",
    "\n",
    "We can clearly see that the margin is narrow and will result in poor generalisation,\n",
    "\n",
    "**Case 2:**\n",
    "\n",
    "The same division when we add another outlier\n",
    "\n",
    "![](../images/soft_margin_3.jpg)\n",
    "\n",
    "\n",
    "In this case the classifier will be unable to find a linear division. \n",
    "\n",
    "\n",
    "All these problems because of a `single` point seems too problematic.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "What if we the linear division looked like this for the above cases?\n",
    "\n",
    "**Case 1:**\n",
    "\n",
    "![](../images/soft_margin_4.jpg)\n",
    "\n",
    "**Case 2:**\n",
    "\n",
    "![](../images/soft_margin_5.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In both the above cases, even though we will get a small misclassifation error but the hyperplane division is generalised enough for new data points to be correctly predicted.\n",
    "\n",
    "In other words, we want to be permissive for certain examples, allowing that their classification by the separator, diverge from the real class.  By doing this we are relaxing the margin and therefore utilising something called a **soft margin**\n",
    "\n",
    "**Soft Margin**\n",
    "The goal of Soft Margin is not to make zero classification mistakes but to make as few mistakes as possible.\n",
    "\n",
    "We know the constraint of `Hard-Margin`(or Normal margin) is defined as :\n",
    "\n",
    "$y_i(w⋅x_i+b)≥1$\n",
    "\n",
    "So when the two classes are not linearly separable (e.g., due to noise), the condition for the optimal hyper-plane can be relaxed by including an extra term: \n",
    "\n",
    "$y_i(w⋅x_i+b)≥1 - ξ_i$\n",
    "\n",
    "$ξ_i$ is what is known as `slack variables`. \n",
    "\n",
    "This variables represent the deviation of the examples from the margin.\n",
    "\n",
    "![](../images/slack_variables.jpg)\n",
    "\n",
    "That results in the optimisation function changing from  \n",
    "\n",
    "$Minimize_{w,b}$ for  $\\frac{1}{2}.||w|| $ \n",
    "\n",
    "subject to $y_i(w⋅x_i+b)≥1$  (for any i=1,…,n)   \n",
    "\n",
    "to\n",
    "\n",
    "$Minimize_{w,b}$ for $\\frac{1}{2}||w||$ +  $\\sum_{i=1}^mξ_i$ \n",
    "\n",
    "subject to $y_i(w⋅x_i+b)≥1-ξ_i$  (for any i=1,…,n)   \n",
    "\n",
    "\n",
    "In other words, we take the sum of each individual $ξ_i$ and add it to our objective function. This is nothing but adding of penalty term as regualarisation. \n",
    "This will result in a hyperplane that maximizes the margin while having the smallest error possible.\n",
    "\n",
    "We will add two more constrains to the above optimisation problem:\n",
    "\n",
    "* Ensure  $ξ_i$ is always greater than $0$(One can easily minimise $ξ_i$ using negative values of it)\n",
    "\n",
    "* Have control over the soft-margin.\n",
    "\n",
    "The final optimisation will look like the following\n",
    "\n",
    "$Minimize_{w,b}$ for $\\frac{1}{2}||w||$ +  $C.\\sum_{i=1}^mξ_i$ \n",
    "\n",
    "subject to $y_i(w⋅x_i+b)≥1-ξ_i$ ($ξ_i>0$) (for any i=1,…,n)\n",
    "\n",
    "\n",
    "The parameter `C` in the above equation is how we control the soft-margin.\n",
    "Let's understand it better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation Parameters\n",
    "\n",
    "**Significance of C:**\n",
    "\n",
    "Following is the final optimisation statement:\n",
    "\n",
    "$Minimize_{w,b}$ for $\\frac{1}{2}||w||$ +  $C.\\sum_{i=1}^mξ_i$ \n",
    "\n",
    "subject to $y_i(w⋅x_i+b)≥1-ξ_i$ ($ξ_i>0$) (for any i=1,…,n)\n",
    "\n",
    "\n",
    "As mentioned before the parameter `C` gives control over our defined soft-margin.\n",
    "\n",
    "Let's see how changing `C` leads to change in hyperplanes.\n",
    "\n",
    "**Case 1:** No noise/Outlier\n",
    "\n",
    "![](../images/C_0.jpg)\n",
    "\n",
    "\n",
    "As C approaches infinity, it is same as having the optimal hard margin classification.\n",
    "\n",
    "Decreasing it further leads to reduction of margin and the hyperplane resulting in being closer to data points. In other words, the generalisation decreases.\n",
    "\n",
    "**Case 2:** Noisy Data\n",
    "\n",
    "![](../images/C_00.jpg)\n",
    "\n",
    "In this case C approaching infinity will lead to no solutions because there doesn't exist a hard margin classifier.\n",
    "\n",
    "Looking at the resulting graph of three different C values, we see that the best result is C=4,\n",
    "though decreasing it further leads to unnecessary misclassifications.\n",
    "\n",
    "\n",
    "\n",
    "**Summary:**\n",
    "* A small value of `C` gives a wider margin, at the cost of some misclassifications.\n",
    "* A large value of `C` gives same result as a hard margin classifier \n",
    "\n",
    "\n",
    "The trade off is to find the optimum value of `C` such that noisy data is tolerated enough and does not affect the solution.\n",
    "\n",
    "Let's see it's python implementation\n",
    "\n",
    "**Python implementation of `C`**\n",
    "\n",
    "It already comes as a hyper-parameter in `sklearn` model.\n",
    "\n",
    "\n",
    "`class sklearn.svm.LinearSVC(C=1.0 , random_state=None)`\n",
    "\n",
    "\n",
    "Let's try experimenting with `C` values for our `phone price` problem statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Kernels?\n",
    "\n",
    "In our first chapter when talking about the `phone price` problem, we had encountered a data distribution like this:\n",
    "\n",
    "![](../images/svm_graph_8.jpg)\n",
    "\n",
    "\n",
    "There we had talked about **Transformation** of data to look something like this:\n",
    "\n",
    "![](../images/svm_graph_9.jpg)\n",
    "\n",
    "This resulted in having a possible linear division of the data points of the two classes.\n",
    "\n",
    "Let's try to understand in detail what this **Transformation** entails.\n",
    "\n",
    "\n",
    "**Transformation to higher space**\n",
    "\n",
    "When we are unable to find a hyperplane dividing the class data points in our `input space`(actual dimension of our data points), it's possible that there exists a higher `Z` space where there is infact a perfect linear hyperplane division.\n",
    "\n",
    "The idea is to transform the data from the input space to a higher dimensional space using a function  $\\phi$(x) \n",
    "\n",
    "This new space is called the feature space\n",
    "The advantage of the transformation is that linear operations in the feature space are equivalent to non-linear operations in the input space.\n",
    "\n",
    "\n",
    "Let's reiterate that using an example:\n",
    "\n",
    "Consider the following graph:\n",
    "\n",
    "\n",
    "![](../images/kernel_1.jpg)\n",
    "\n",
    "There's no line that we can draw to separate the two classes.\n",
    "\n",
    "Let's define a non-linear mapping function from the 2-dimensional input space I into the 3-\n",
    "dimensional feature space Z, which is defined in the following way:\n",
    "$\\phi(x) = (x_1^2,\\sqrt2x_1x_2, x_2^2)^T$.\n",
    "\n",
    "If we apply the above function to our data, it will look similar to:\n",
    "\n",
    "![](../images/kernel_2.jpg)\n",
    "\n",
    "In this dimension though, we find that we have infact a linear separating hyperplane.\n",
    "\n",
    "\n",
    "Suppose taking the equation for a separating hyperplane into account we get a linear function in 3-D like this:\n",
    "\n",
    "$\\phi(x) = w_1x_1^2 + w_2\\sqrt2x_1x_2 + w_3x_2^2 = 0$\n",
    "\n",
    "\n",
    "What's interesting to note is that the same linear function in 3-D space is an elliptic function in 2-D space.\n",
    "\n",
    "If we try transforming the linear hyperplane back to 2-D space, we will end up with something similar to:\n",
    "\n",
    "![](../images/kernel_3.jpg)\n",
    "\n",
    "Hence, with an appropriate mapping function we can use our linear classifier in feature space Z on a transformed version of the data to get a non-linear classifier in input space I with the same SVM principles we just learned. \n",
    "\n",
    "***\n",
    "\n",
    "Let's reiterate our theory:\n",
    "\n",
    "` When we are unable to find a hyperplane dividing the class data points in our 'input space'(actual dimension of our data points), all we need to do is find a higher 'feature space' where there is a linear hyperplane division.`\n",
    "\n",
    "The theory though completely sound has one major problem:\n",
    "\n",
    "This quick solution of transforming non-separable dataset is computationally expensive.\n",
    "Transformation from 2-D to 3D is ok. But what if we need to transform it to a n'th dimension?\n",
    "Even for simple transformation from 2-D to 3D, what if your data has millions of samples?\n",
    "\n",
    "This method therefore is clearly not scalable. **Enter Kernels**\n",
    "\n",
    "**What is Kernel?**\n",
    "\n",
    "We didn't get into the maths of optimisation in the last part but one good thing to know is that when we transform the following formulation: \n",
    "\n",
    "$Minimize_{w,b}$ for $\\frac{1}{2}.||w|| $ \n",
    "\n",
    "subject to $y_i(w⋅x_i+b)≥1$  (for any i=1,…,n)   \n",
    "\n",
    "We end up with the following equation:\n",
    "***\n",
    "$L(a):\\sum_{n=1}^N\\alpha_n - \\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{N}y_ny_m\\alpha_n\\alpha_mx_n^Tx_m$\n",
    "\n",
    "where \n",
    "$\\alpha_n,\\alpha_m$ are known as **Lagrange multipliers**,\n",
    "\n",
    "$y_n$ and $y_m$ are your target class points,\n",
    "\n",
    "$x_n$ and $x_m$ are your data points.\n",
    "***\n",
    "\n",
    "One thing to note from the above equation is we don't need the value of training examples. We only need the value of dot product between $x_n$ and $x_m$\n",
    "\n",
    "This holds true even when we go to a higher feature space `Z`,\n",
    "\n",
    "$L(a):\\sum_{n=1}^N\\alpha_n - \\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{N}y_ny_m\\alpha_n\\alpha_mz_n^Tz_m$\n",
    "\n",
    "So to solve our problem, what we need is function that returns the result of a inner dot product performed in another space. That function is called `kernel`.\n",
    "\n",
    "Informally Kernel function is defined as `some function that corresponds to an inner product in some expanded feature space'.` \n",
    "\n",
    "Let's try to understand `Kernel function` better with an example:\n",
    "\n",
    "Let's assume we have data point x=[x1,x2] which when transformed to a higher feature space `Z` is defined as \n",
    "\n",
    "$\\phi(x)=\\phi(\\begin{bmatrix} x1  \\\\ x2  \\\\ \\end{bmatrix}) =(1, x_1, x_2, x_1^2, x_2^2, x_1x_2)$\n",
    "\n",
    "Inner product in that feature space would be:\n",
    "\n",
    "$\\phi(x,y)=\\phi(\\begin{bmatrix} x1  \\\\ x2  \\\\ \\end{bmatrix}, \\begin{bmatrix} y1  \\\\ y2  \\\\ \\end{bmatrix}) = (1 + x_1.y_1 + x_2.y_2 + x_1^2.y_1^2 + x_2^2.y_2^2 + x_1.y_1.x_2.y_2)$\n",
    "\n",
    "**Note:** All we have done for the inner product is substitute the terms for $\\phi$ function and multiply the corresponding terms.\n",
    "\n",
    "More formally we can write,\n",
    "\n",
    "$A(x,y)= (1 + x_1.y_1 + x_2.y_2 + x_1^2.y_1^2 + x_2^2.y_2^2 + x_1.y_1.x_2.y_2)$\n",
    "\n",
    "where `A` is the function calculating transforming `x` and `y` and calculating the inner product of the transformed `x` and `y`\n",
    "\n",
    "**Can we compute this without transforming `x` and `y`?**\n",
    "\n",
    "\n",
    "Yes, using something called **Kernel Trick**\n",
    "\n",
    "\n",
    "Suppose you are given,\n",
    "\n",
    "$K(x,y)=(1 + x^Ty)^2$\n",
    "\n",
    "The above function is not an explicit inner product function or involves any sort of transformation of `x` and `y` to a higher feature space.\n",
    "\n",
    "On expanding it we get the following value of kernel,\n",
    "\n",
    "$K(x,y)=(1 + x^T.y)^2=(1 + x_1.y_1 + x_2.y_2)^2=1 + x_1^2.y_1^2 + x_2^2.y_2^2 + 2x_1y_1 + 2x_2y_2 + 2x_1y_1x_2y_2$\n",
    "\n",
    "The above value looks very similar to the transformed inner product we calculated for `x` and `y` except for presence of `2` in the last three terms of the value.\n",
    "\n",
    "So would that mean it is not an inner product of the transformed `x` and `y`?\n",
    "No.\n",
    "\n",
    "It is the inner product if our $\\phi(x)$ was equal to $(1,\\sqrt2x_1, \\sqrt2x_2, x_1^2, x_2^2, \\sqrt2x_1x_2)$\n",
    "\n",
    "**Q:** In our case calculating inner product using the `A` or `K` isn't really that different, so how are kernels helping?\n",
    "\n",
    "**A:** \n",
    "The big difference between A and K is when we expand it to a higher dimension than 2.\n",
    "\n",
    "Suppose for the same data you wanted to expand it to a space with polynomial of order `Q`\n",
    "\n",
    "The equivalent kernel K would be: \n",
    "\n",
    "$K(x,y)=(1+ x^T.y)=(1+ x_1y_1+ x_2y_2)^Q$\n",
    "\n",
    "The calculation inside the bracket has the same complexity as the dimensions of our input space `I`(2 in our case). \n",
    "\n",
    "The only part that is left is calculation of power `Q` which can easily be calculated by getting the logarithmic value, multiplying it with `Q` and then exponentiating it back. That means any value of `Q` would result in the same complexity.\n",
    "\n",
    "\n",
    "Compare that with the first function. You will get a tremendous no. of terms and all sorts of permutation and combinations of those terms depending on the value of `Q`.\n",
    "\n",
    "This is the **kernel trick**\n",
    "\n",
    "***\n",
    "We can therefore replace the following equation:\n",
    "\n",
    "$L(a):\\sum_{n=1}^N\\alpha_n - \\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{N}y_ny_m\\alpha_n\\alpha_mz_n^Tz_m$\n",
    "\n",
    "with it's modified version:\n",
    "\n",
    "$L(a):\\sum_{n=1}^N\\alpha_n - \\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{N}y_ny_m\\alpha_n\\alpha_mK(x_n^Tx_m)$\n",
    "\n",
    "\n",
    "\n",
    "This change looks very simple, but leads us to a much better working of SVM.\n",
    "\n",
    "To make you appreciate it better, suppose you had a high dimension data(n>1000). While finding an optimal hyperplane for the target variable at a higher dimension `Z`.\n",
    "\n",
    "**Without Kernel**\n",
    "\n",
    "In that case one will have to first project the data points in the `Z` dimension, calculate the dot product of the points in the `Z dimension` and return the dot product back to the input data dimension.\n",
    "\n",
    "**With Kernel**\n",
    "\n",
    "Take the datapoints and calculate Kernel dot product of the points. That's it.\n",
    "\n",
    "\n",
    "Another beauty of using kernel in SVM is that all$K(x,y)$ has to do, is give us an inner product of support vectors and not any other vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Kernels\n",
    "\n",
    "Kernel is some function corresponding to inner product of a feature space `Z`.\n",
    "\n",
    "That means there `exists` infinite kernels for you to choose from. Additionaly you can create your own kernels as well.\n",
    "\n",
    "Thankfully, in practice a couple of kernels turned out to be the most appropriate for most of the common settings. Let's look at them one by one:\n",
    "\n",
    "1. **Linear Kernel**\n",
    "\n",
    "It is defined as: \n",
    "\n",
    "$K(x,y)=x^Ty$\n",
    "\n",
    "This is the simplest kernel. \n",
    "\n",
    "This is how SVM operates by default.\n",
    "\n",
    "2. **Polynomial Kernel**\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$K(x,y)=(x^Ty +c)^q$\n",
    "\n",
    "We have already encountered a modified version of it($(1+ x^T.y)$) while understanding the kernel trick.\n",
    "\n",
    "It has 2 parameters c(constant) and q(degree of kernel).\n",
    "\n",
    "A polynomial kernel with q=1 is same as linear kernel.\n",
    "\n",
    "For eg:\n",
    "\n",
    "Consider the same data distribution like before:\n",
    "\n",
    "![](../images/kernel_1.jpg)\n",
    "\n",
    "\n",
    "Following will be the different hyperplanes for the different values of q:\n",
    "\n",
    "![](../images/poly_0.jpg)\n",
    "\n",
    "You can clearly see higher the q, higher the overfitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. **Radial Basis Function(RBF) kernel/ Gaussian Kernel**\n",
    "\n",
    "\n",
    "Both polynomial kernel and linear kernel are still simple kernels. There are complex cases where both will fail.\n",
    "\n",
    "For eg: Look at the following familiar distribution\n",
    "\n",
    "![](../images/rbf_1.jpg)\n",
    "\n",
    "When solved using polynomial kernel, it would look something like:\n",
    "\n",
    "\n",
    "![](../images/rbf_2.jpg)\n",
    "\n",
    "A kernel that can help us in this case is RBF Kernel\n",
    "\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$K(x,y)=exp(-\\gamma||x-y||^2)$\n",
    "\n",
    "where $\\gamma$ is one of the regularisation parameters.\n",
    "\n",
    "It is powerful because it projects vectors into an infinite dimensional space.\n",
    "\n",
    "Yes, infinite.\n",
    "\n",
    "**How?**\n",
    "\n",
    "Without going into the mathematical proof behind it, Taking $\\gamma$ =-1, following is the function's expansion:\n",
    "\n",
    "$exp(-x^2)exp(-y^2)\\sum_{k=0}^\\infty\\frac{2^k(x^k)(y^k)}{k!}$\n",
    "\n",
    "RBF kernel is formed by taking an infinite sum over polynomial kernels.\n",
    "\n",
    "\n",
    "Following is how the decision boundary formed by RBF(with $\\gamma$=0.2):\n",
    "\n",
    "![](../images/rbf_3.jpg)\n",
    "\n",
    "**Effect of Gamma**\n",
    "\n",
    "The gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’.\n",
    "\n",
    "Here influence means far and close refers to how much generalisation each support vector provides.\n",
    "\n",
    "If gamma is too large, the radius of the area of influence of the support vectors only includes the support vector itself.\n",
    "\n",
    "When gamma is very small, the model is too constrained and cannot capture the complexity or “shape” of the data. The region of influence of any selected support vector would include the whole training set. The resulting model will behave similarly to a linear model.\n",
    "\n",
    "Simply put higher the value of gamma, higher the influence will be by each support vector.\n",
    "\n",
    "Following are the two different decision boundaries with two different gamma values:\n",
    "\n",
    "![](../images/rbf_0.jpg)\n",
    "\n",
    "\n",
    "\n",
    "**Summary table**\n",
    "\n",
    "|Feature|Order|\n",
    "|-----|-----|\n",
    "|time of SVM learning| linear < poly < rbf|\n",
    "|ability to fit any data| linear < poly < rbf\n",
    "|risk of overfitting| linear < poly < rbf|\n",
    "|risk of underfitting| rbf < poly < linear|\n",
    "|number of hyperparameters| linear (0) < rbf (2) < poly (3)|\n",
    "\n",
    "\n",
    "Let's see it's python implementation\n",
    "\n",
    "**Python implementation of `kernels`**\n",
    "\n",
    "Instead of using `LinearSVC`,we will use `SVC` module of `sklearn.svm` library.\n",
    "\n",
    "In `SVC`, it already comes as a hyper-parameter \n",
    "\n",
    "`class sklearn.svm.SVC(kernel='rbf', C=1.0 , random_state=None)`\n",
    "\n",
    "\n",
    "Let's try solving our `phone price` problem statement using `polynomial` and `rbf` kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Kernels\n",
    "\n",
    "\n",
    "**For Polynomial Kernel**\n",
    "- Initialise a SVM model with `SVM()` with `random_state=0`, `kernel=poly` and save it to a variable called `'poly_model'`.\n",
    "\n",
    "\n",
    "- Fit the model `'poly_model'` on the training data `'X_train'` and `'y_train'` using the `'fit()'` method.\n",
    "\n",
    "\n",
    "- Find out the accuracy between `X_test` and `'y_test'` using the `'score()'` method of `'poly_model'` and store it in a variable called `'acc_poly'`\n",
    "\n",
    "**For RBF Kernel**\n",
    "\n",
    "- Initialise a SVM model with `SVM()` with `random_state=0`, `kernel=rbf` and save it to a variable called `'rbf_model'`.\n",
    "\n",
    "\n",
    "- Fit the model `'rbf_model'` on the training data `'X_train'` and `'y_train'` using the `'fit()'` method.\n",
    "\n",
    "\n",
    "- Find out the accuracy between `X_test` and `'y_test'` using the `'score()'` method of `'rbf_model'` and store it in a variable called `'acc_rbf'`\n",
    "\n",
    "\n",
    "- Compare both accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions to SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass SVM\n",
    "\n",
    "We know SVM are efficiently able to generate binary classifiers. Unfortunately there are often datasets which have more than two classes.\n",
    "\n",
    "Let's take an extension of our phone price problem:\n",
    "\n",
    "What if instead of 0(low end) and 1(high end) phones we had the following four classes:\n",
    "\n",
    "* 0 [price range< 10K]\n",
    "* 1 [price range> 10K & price range< 20K]\n",
    "* 2 [price range> 20K & price range< 30K]\n",
    "* 3 [price range> 30K & price range< 40K]\n",
    "\n",
    "There are many approaches that allows SVM to extend it's binary class approach to multi class approach. Following are the most popular ones:\n",
    "\n",
    "**One vs All(OVA)**\n",
    "\n",
    "This is the most simple(and naive) approach. \n",
    "\n",
    "\n",
    "Consider the following class distribution:\n",
    "\n",
    "![](../images/mc_1.jpg)\n",
    "\n",
    "In order to classify the four classes, we will construct four different binary classifiers. \n",
    "\n",
    "For a given class, the positive examples are all the points in the class, and the negative examples are all the points of the other class.As a result we obtain one binary classifier for each problem.\n",
    "\n",
    "Following are the four different binary classifiers, we will get:\n",
    "\n",
    "![](../images/mc_2.jpg)\n",
    "\n",
    "Which will result in the following final boundaries\n",
    "\n",
    "![](../images/mc_3.jpg)\n",
    "\n",
    "For every new prediction, we use each classifer and return the classifier which returns a positive result.\n",
    "\n",
    "**Drawback**\n",
    "\n",
    "* Can lead to multiple classes\n",
    "\n",
    "* Can lead to no class\n",
    "\n",
    "One heuristic to avoid these ambiguities is to assign it based on the maximum value that the classifier will give out instead of the absolute value(i.e. Instead of just assigning class based on whether the result is positive or negative, assign it based on the actual positive or negative value).\n",
    "\n",
    "That helps this approach choose the class of the hyperplane that is closest to the example when all classifiers disagree. \n",
    "\n",
    "Following would be the division of hyperplanes in that case:\n",
    "\n",
    "![](../images/mc_4.jpg)\n",
    "\n",
    "Even after that, this method doesn't always give satisfactory results.\n",
    "\n",
    "Still, it does remain a popular method for multi-class classification owing to its easy implementation and understandability\n",
    "\n",
    "**In sklearn implementation, LinearSVC implements this strategy by default**\n",
    "\n",
    "**One vs One**\n",
    "\n",
    "This approach is a upgraded version of OVA, where instead of differentiating one class from all the other classifers, we try to distinguish one class from every other other class.\n",
    "\n",
    "As a result, we train one classifier per pair of classes, which leads to a total of K(K-1)/2 classifiers for K classes. \n",
    "\n",
    "Predictions are then made using `voting`. Each new instance is passed to each classifier and the predicted class is recorded. Then the class having the most votes is assigned to the instance.\n",
    "\n",
    "Following are the six different binary classifiers, we will get:\n",
    "\n",
    "\n",
    "![](../images/mc_5.jpg)\n",
    "\n",
    "Which will result in the following final boundaries\n",
    "\n",
    "![](../images/mc_6.jpg)\n",
    "\n",
    "Let's see the python implementation of the above two:\n",
    "\n",
    "**Python implementation of `multi-class`**\n",
    "\n",
    "In `SVC`, it already comes as a hyper-parameter \n",
    "\n",
    "`class sklearn.svm.SVC(kernel='rbf', C=1.0 , decision_function_shape='ova', random_state=None)`\n",
    "\n",
    "Let's try and implement it to solve our modified phone problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary    \n",
    "Let's sum up all that we have learned of SVM in terms of its performance.\n",
    "\n",
    "**Advantages**\n",
    "- Having the flexibility to choose kernel, SVM  works well on a wide range of classification problems, even problems in high dimensions and that are not linearly separable.\n",
    "\n",
    "\n",
    "- SVM models have generalization(regularisation parameters) in practice, therefore the risk of overfitting is less in SVM.\n",
    "\n",
    "\n",
    "- SVM works well with even unstructured and semi structured data like text, images and trees.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- Optimisation of key parameters needs to be done for each problem to get satisfactorily results. Parameters that may result in an excellent classification accuracy for problem A, may result in a poor classification accuracy for problem B.\n",
    "\n",
    "\n",
    "- Design of Multiclass SVM classifiers(OVA and OVO require multiple SVM instances) are still not optimal enough for practical applications\n",
    "\n",
    "\n",
    "- The quadratic programming problem(primal formulation) is computationally intensive and with increase in training data it can be very slow on normal machines\n",
    "\n",
    "\n",
    "- SVM in its pure implementation cannot return a probabilistic confidence value(class probabilities) like another classifier say Logistic Regression. This confidence of prediction is sometimes more important than the actual prediction in many applications\n",
    "\n",
    "***\n",
    "Despite being powerful in theory and practice, it isn't being used extensively today. \n",
    "\n",
    "On unstructured data, it is easily outperformed by neural networks and on structured data, by gradient boosted trees. Still it's an algorithm worth having in your ML toolset owing to its radical and intuitive approach of solving ML problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"ppt-icons\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Mini Challenge - 1\n",
    "***\n",
    "\n",
    "- Load the dataset `wbc.csv` from the data folder\n",
    "- Do a label Encoding on the target column **diagnosis**\n",
    "- Drop the columns **Unnamed: 32** and **id** and store the resultant into a variable called **df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"ppt-icons\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Mini Challenge - 2\n",
    "***\n",
    "\n",
    "- Fit a `MinMaxScaler()` on the DataFrame **df**\n",
    "- Store the predictors in **X** and the target in **y** \n",
    "- Do a `train_test_split()` on X,y with `test_size=0.3`, `random_state=42`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"ppt-icons\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Mini Challenge - 3\n",
    "***\n",
    "\n",
    "- Fit an SVC model with `kernel='linear'`\n",
    "- Print out the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9883040935672515"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"ppt-icons\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Mini Challenge - 4\n",
    "***\n",
    "\n",
    "- Fit an SVC model with `kernel='rbf'`\n",
    "- Print out the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9590643274853801"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"ppt-icons\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Mini Challenge - 5\n",
    "***\n",
    "\n",
    "- Fit a `GridSearchCV()` with parameters:\n",
    "    - C - `[0.01, 0.1, 0.001, 1, 10, 100]`\n",
    "    - gamma - `[0.0001, 0.001, 0.01, 0.1]`\n",
    "    - kernel - `['linear','rbf']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"ppt-icons\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Mini Challenge - 6\n",
    "***\n",
    "- Print out the best estimators and the best score for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
