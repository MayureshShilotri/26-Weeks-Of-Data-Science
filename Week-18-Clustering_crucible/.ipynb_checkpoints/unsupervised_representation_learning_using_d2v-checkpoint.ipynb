{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning with Paragraph Vector algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsupervised Learning\n",
    "\n",
    "Unsupervised learning is the training of an artificial intelligence (AI) algorithm using information that is neither classified nor labeled and allowing the algorithm to act on that information without guidance.\n",
    "\n",
    "<img src=\"./unsupervised_examples.png\">\n",
    "\n",
    "### Unsupervised Techniques\n",
    "* Clustering\n",
    "* Principal Component Analysis (PCA)\n",
    "* Anomaly detection\n",
    "* Autoencoders\n",
    "* Deep Belief Nets\n",
    "* Hebbian Learning\n",
    "* Generative Adversarial Networks(GANs)\n",
    "* Self-Organizing maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./representation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Vector algorithm (also known as Document To Vector/D2V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./para_vec_new.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it?\n",
    "\n",
    "Doc2Vec is an NLP tool for representing documents as a vector and is a generalizing of the Word2Vec method. This tutorial will serve as an introduction to Doc2Vec and present ways to train and assess a Doc2Vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [Word2Vec Paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* [Doc2Vec Paper](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)\n",
    "* [Dr. Michael D. Lee's Website](http://faculty.sites.uci.edu/mdlee)\n",
    "* [Lee Corpus](http://faculty.sites.uci.edu/mdlee/similarity-data/)\n",
    "* [IMDB Doc2Vec Tutorial](doc2vec-IMDB.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get going, we'll need to have a set of documents to train our doc2vec model. In theory, a document could be anything from a short 140 character tweet, a single paragraph (i.e., journal article abstract), a news article, or a book. In NLP parlance a collection or set of documents is often referred to as a <b>corpus</b>. \n",
    "\n",
    "For this tutorial, we'll be training our model using the [Lee Background Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) included in gensim. This corpus contains 314 documents selected from the Australian Broadcasting\n",
    "Corporation’s news mail service, which provides text e-mails of headline stories and covers a number of broad topics.\n",
    "\n",
    "And we'll test our model by eye using the much shorter [Lee Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) which contains 50 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
    "lee_test_file = test_data_dir + os.sep + 'lee.cor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Read and Preprocess Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a function to open the train/test file (with latin encoding), read the file line-by-line, pre-process each line using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc), and return a list of words. Note that, for a given file (aka corpus), each continuous line constitutes a single document and the length of each line (i.e., document) can vary. Also, to train the model, we'll need to associate a tag/number with each document of the training corpus. In our case, the tag is simply the zero-based line number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]),\n",
       " TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'national', 'executive', 'of', 'the', 'strife', 'torn', 'democrats', 'last', 'night', 'appointed', 'little', 'known', 'west', 'australian', 'senator', 'brian', 'greig', 'as', 'interim', 'leader', 'shock', 'move', 'likely', 'to', 'provoke', 'further', 'conflict', 'between', 'the', 'party', 'senators', 'and', 'its', 'organisation', 'in', 'move', 'to', 'reassert', 'control', 'over', 'the', 'party', 'seven', 'senators', 'the', 'national', 'executive', 'last', 'night', 'rejected', 'aden', 'ridgeway', 'bid', 'to', 'become', 'interim', 'leader', 'in', 'favour', 'of', 'senator', 'greig', 'supporter', 'of', 'deposed', 'leader', 'natasha', 'stott', 'despoja', 'and', 'an', 'outspoken', 'gay', 'rights', 'activist'], ['cash', 'strapped', 'financial', 'services', 'group', 'amp', 'has', 'shelved', 'million', 'plan', 'to', 'buy', 'shares', 'back', 'from', 'investors', 'and', 'will', 'raise', 'million', 'in', 'fresh', 'capital', 'after', 'profits', 'crashed', 'in', 'the', 'six', 'months', 'to', 'june', 'chief', 'executive', 'paul', 'batchelor', 'said', 'the', 'result', 'was', 'solid', 'in', 'what', 'he', 'described', 'as', 'the', 'worst', 'conditions', 'for', 'stock', 'markets', 'in', 'years', 'amp', 'half', 'year', 'profit', 'sank', 'per', 'cent', 'to', 'million', 'or', 'share', 'as', 'australia', 'largest', 'investor', 'and', 'fund', 'manager', 'failed', 'to', 'hit', 'projected', 'per', 'cent', 'earnings', 'growth', 'targets', 'and', 'was', 'battered', 'by', 'falling', 'returns', 'on', 'share', 'markets']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain any tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a Doc2Vec Object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll instantiate a Doc2Vec model with a vector size with 50 words and iterating over the training corpus 40 times. We set the minimum word count to 2 in order to discard words with very few occurrences. (Without a variety of representative examples, retaining such infrequent words can often make a model worse!) Typical iteration counts in published 'Paragraph Vectors' results, using 10s-of-thousands to millions of docs, are 10-20. More iterations take more time and eventually reach a point of diminishing returns.\n",
    "\n",
    "However, this is a very very small dataset (300 documents) with shortish documents (a few hundred words). Adding training passes can sometimes help with such small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=48, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-22 14:27:36,331 : INFO : collecting all words and their counts\n",
      "2019-02-22 14:27:36,338 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-02-22 14:27:36,375 : INFO : collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "2019-02-22 14:27:36,376 : INFO : Loading a fresh vocabulary\n",
      "2019-02-22 14:27:36,401 : INFO : effective_min_count=2 retains 3955 unique words (56% of original 6981, drops 3026)\n",
      "2019-02-22 14:27:36,403 : INFO : effective_min_count=2 leaves 55126 word corpus (94% of original 58152, drops 3026)\n",
      "2019-02-22 14:27:36,431 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2019-02-22 14:27:36,433 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2019-02-22 14:27:36,435 : INFO : downsampling leaves estimated 42390 word corpus (76.9% of prior 55126)\n",
      "2019-02-22 14:27:36,470 : INFO : estimated required memory for 3955 words and 48 dimensions: 3553820 bytes\n",
      "2019-02-22 14:27:36,478 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via `model.wv.vocab`) of all of the unique words extracted from the training corpus along with the count (e.g., `model.wv.vocab['penalty'].count` for counts for the word `penalty`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-22 14:27:37,352 : INFO : training model with 3 workers on 3955 vocabulary and 48 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-02-22 14:27:37,513 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:37,522 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:37,547 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:37,549 : INFO : EPOCH - 1 : training on 58152 raw words (42772 effective words) took 0.2s, 227759 effective words/s\n",
      "2019-02-22 14:27:37,713 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:37,731 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:37,745 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:37,746 : INFO : EPOCH - 2 : training on 58152 raw words (42568 effective words) took 0.2s, 224132 effective words/s\n",
      "2019-02-22 14:27:37,918 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:37,933 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:37,943 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:37,946 : INFO : EPOCH - 3 : training on 58152 raw words (42723 effective words) took 0.2s, 218474 effective words/s\n",
      "2019-02-22 14:27:38,099 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:38,127 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:38,132 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:38,134 : INFO : EPOCH - 4 : training on 58152 raw words (42704 effective words) took 0.2s, 246890 effective words/s\n",
      "2019-02-22 14:27:38,281 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:38,327 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:38,330 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:38,332 : INFO : EPOCH - 5 : training on 58152 raw words (42701 effective words) took 0.2s, 219453 effective words/s\n",
      "2019-02-22 14:27:38,511 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:38,533 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:38,536 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:38,538 : INFO : EPOCH - 6 : training on 58152 raw words (42773 effective words) took 0.2s, 212244 effective words/s\n",
      "2019-02-22 14:27:38,701 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:38,717 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:38,723 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:38,725 : INFO : EPOCH - 7 : training on 58152 raw words (42716 effective words) took 0.2s, 252589 effective words/s\n",
      "2019-02-22 14:27:38,902 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:38,913 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:38,914 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:38,915 : INFO : EPOCH - 8 : training on 58152 raw words (42694 effective words) took 0.2s, 230892 effective words/s\n",
      "2019-02-22 14:27:39,053 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:39,082 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:39,099 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:39,100 : INFO : EPOCH - 9 : training on 58152 raw words (42811 effective words) took 0.2s, 237066 effective words/s\n",
      "2019-02-22 14:27:39,249 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:39,260 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:39,275 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:39,277 : INFO : EPOCH - 10 : training on 58152 raw words (42685 effective words) took 0.2s, 247092 effective words/s\n",
      "2019-02-22 14:27:39,459 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:39,480 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:39,485 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:39,486 : INFO : EPOCH - 11 : training on 58152 raw words (42627 effective words) took 0.2s, 207520 effective words/s\n",
      "2019-02-22 14:27:39,691 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:39,717 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:39,727 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:39,731 : INFO : EPOCH - 12 : training on 58152 raw words (42691 effective words) took 0.2s, 194542 effective words/s\n",
      "2019-02-22 14:27:39,876 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:39,888 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:39,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:39,893 : INFO : EPOCH - 13 : training on 58152 raw words (42651 effective words) took 0.2s, 270647 effective words/s\n",
      "2019-02-22 14:27:40,121 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:40,133 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:40,141 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:40,143 : INFO : EPOCH - 14 : training on 58152 raw words (42652 effective words) took 0.2s, 175891 effective words/s\n",
      "2019-02-22 14:27:40,333 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:40,337 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:40,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:40,352 : INFO : EPOCH - 15 : training on 58152 raw words (42696 effective words) took 0.2s, 207928 effective words/s\n",
      "2019-02-22 14:27:40,504 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:40,528 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:40,564 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:40,569 : INFO : EPOCH - 16 : training on 58152 raw words (42667 effective words) took 0.2s, 207647 effective words/s\n",
      "2019-02-22 14:27:40,745 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:40,754 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:40,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:40,757 : INFO : EPOCH - 17 : training on 58152 raw words (42662 effective words) took 0.2s, 250133 effective words/s\n",
      "2019-02-22 14:27:40,931 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:40,959 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:40,967 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:40,969 : INFO : EPOCH - 18 : training on 58152 raw words (42692 effective words) took 0.2s, 205118 effective words/s\n",
      "2019-02-22 14:27:41,191 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:41,220 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:41,226 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:41,229 : INFO : EPOCH - 19 : training on 58152 raw words (42708 effective words) took 0.2s, 196771 effective words/s\n",
      "2019-02-22 14:27:41,431 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:41,449 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:41,455 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:41,457 : INFO : EPOCH - 20 : training on 58152 raw words (42724 effective words) took 0.2s, 205092 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-22 14:27:41,685 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:41,697 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:41,710 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:41,712 : INFO : EPOCH - 21 : training on 58152 raw words (42790 effective words) took 0.2s, 171269 effective words/s\n",
      "2019-02-22 14:27:41,917 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:41,922 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:41,941 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:41,942 : INFO : EPOCH - 22 : training on 58152 raw words (42561 effective words) took 0.2s, 192684 effective words/s\n",
      "2019-02-22 14:27:42,086 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:42,100 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:42,127 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:42,130 : INFO : EPOCH - 23 : training on 58152 raw words (42732 effective words) took 0.2s, 233535 effective words/s\n",
      "2019-02-22 14:27:42,273 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:42,297 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:42,307 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:42,310 : INFO : EPOCH - 24 : training on 58152 raw words (42659 effective words) took 0.2s, 246575 effective words/s\n",
      "2019-02-22 14:27:42,434 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:42,448 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:42,456 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:42,457 : INFO : EPOCH - 25 : training on 58152 raw words (42619 effective words) took 0.1s, 307445 effective words/s\n",
      "2019-02-22 14:27:42,600 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:42,619 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:42,620 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:42,623 : INFO : EPOCH - 26 : training on 58152 raw words (42630 effective words) took 0.2s, 277353 effective words/s\n",
      "2019-02-22 14:27:42,754 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:42,764 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:42,768 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:42,769 : INFO : EPOCH - 27 : training on 58152 raw words (42749 effective words) took 0.1s, 311436 effective words/s\n",
      "2019-02-22 14:27:42,886 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:42,898 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:42,905 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:42,905 : INFO : EPOCH - 28 : training on 58152 raw words (42507 effective words) took 0.1s, 327312 effective words/s\n",
      "2019-02-22 14:27:43,029 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:43,039 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:43,046 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:43,046 : INFO : EPOCH - 29 : training on 58152 raw words (42670 effective words) took 0.1s, 312495 effective words/s\n",
      "2019-02-22 14:27:43,162 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:43,175 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:43,188 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:43,189 : INFO : EPOCH - 30 : training on 58152 raw words (42756 effective words) took 0.1s, 309037 effective words/s\n",
      "2019-02-22 14:27:43,313 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:43,341 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:43,355 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:43,357 : INFO : EPOCH - 31 : training on 58152 raw words (42647 effective words) took 0.2s, 272978 effective words/s\n",
      "2019-02-22 14:27:43,492 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:43,501 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:43,518 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:43,519 : INFO : EPOCH - 32 : training on 58152 raw words (42647 effective words) took 0.2s, 282911 effective words/s\n",
      "2019-02-22 14:27:43,652 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:43,660 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:43,667 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:43,667 : INFO : EPOCH - 33 : training on 58152 raw words (42661 effective words) took 0.1s, 298496 effective words/s\n",
      "2019-02-22 14:27:43,761 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:43,781 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:43,787 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:43,789 : INFO : EPOCH - 34 : training on 58152 raw words (42723 effective words) took 0.1s, 362786 effective words/s\n",
      "2019-02-22 14:27:43,921 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:43,925 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:43,933 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:43,935 : INFO : EPOCH - 35 : training on 58152 raw words (42717 effective words) took 0.1s, 305136 effective words/s\n",
      "2019-02-22 14:27:44,079 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:44,091 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:44,097 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:44,097 : INFO : EPOCH - 36 : training on 58152 raw words (42748 effective words) took 0.2s, 269888 effective words/s\n",
      "2019-02-22 14:27:44,228 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:44,237 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:44,238 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:44,241 : INFO : EPOCH - 37 : training on 58152 raw words (42742 effective words) took 0.1s, 307556 effective words/s\n",
      "2019-02-22 14:27:44,387 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:44,413 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:44,425 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:44,431 : INFO : EPOCH - 38 : training on 58152 raw words (42692 effective words) took 0.2s, 237201 effective words/s\n",
      "2019-02-22 14:27:44,572 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:44,580 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:44,584 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:44,586 : INFO : EPOCH - 39 : training on 58152 raw words (42631 effective words) took 0.1s, 285051 effective words/s\n",
      "2019-02-22 14:27:44,759 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-22 14:27:44,764 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-22 14:27:44,769 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-22 14:27:44,770 : INFO : EPOCH - 40 : training on 58152 raw words (42733 effective words) took 0.2s, 279568 effective words/s\n",
      "2019-02-22 14:27:44,775 : INFO : training on a 2326080 raw words (1707531 effective words) took 7.4s, 230168 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to note is that you can now infer a vector for any piece of text without having to re-train the model by passing a list of words to the `model.infer_vector` function. This vector can then be compared with other vectors via cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10797174,  0.19715056,  0.0972636 , -0.06634831, -0.05573022,\n",
       "       -0.22814459, -0.01247532, -0.19113724, -0.2335291 , -0.13367285,\n",
       "       -0.04636728, -0.13835841, -0.13360482,  0.17536612,  0.05678298,\n",
       "        0.03886212,  0.09036909,  0.08988244, -0.3042306 , -0.06391596,\n",
       "        0.06544841, -0.04593709, -0.01873477,  0.2733597 ,  0.06608206,\n",
       "       -0.01115317, -0.2598342 ,  0.06903029,  0.05787918, -0.03917908,\n",
       "        0.06389979, -0.24206126, -0.11864331, -0.2917862 , -0.08169527,\n",
       "       -0.06216768,  0.1248833 , -0.08139188,  0.18219376,  0.30983114,\n",
       "       -0.07212948, -0.0461818 ,  0.01162641, -0.01393203, -0.20548104,\n",
       "       -0.19370623, -0.02727796, -0.03386957], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `infer_vector()` does *not* take a string, but rather a list of string tokens, which should have already been tokenized the same way as the `words` property of original training document objects. \n",
    "\n",
    "Also note that because the underlying training/inference algorithms are an iterative approximation problem that makes use of internal randomization, repeated inferences of the same text will return slightly different vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess our new model, we'll first infer new vectors for each document of the training corpus, compare the inferred vectors with the training corpus, and then returning the rank of the document based on self-similarity. Basically, we're pretending as if the training corpus is some new unseen data and then seeing how they compare with the trained model. The expectation is that we've likely overfit our model (i.e., all of the ranks will be less than 2) and so we should be able to find similar documents very easily. Additionally, we'll keep track of the second ranks for a comparison of less similar documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-14 17:54:02,191 : INFO : precomputing L2-norms of doc weight vectors\n",
      "/home/pranav/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how each document ranks with respect to the training corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 292, 1: 8})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(ranks)  # Results vary between runs due to random seeding and very small corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most similar to itself and about 5% of the time it is mistakenly most similar to another document. the checking of an inferred-vector against a training-vector is a sort of 'sanity check' as to whether the model is behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (299): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d48,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (299, 0.9320240020751953): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
      "\n",
      "SECOND-MOST (112, 0.8040372133255005): «australian cricket captain steve waugh has supported fast bowler brett lee after criticism of his intimidatory bowling to the south african tailenders in the first test in adelaide earlier this month lee was fined for giving new zealand tailender shane bond an unsportsmanlike send off during the third test in perth waugh says tailenders should not be protected from short pitched bowling these days you re earning big money you ve got responsibility to learn how to bat he said mean there no times like years ago when it was not professional and sort of bowlers code these days you re professional our batsmen work very hard at their batting and expect other tailenders to do likewise meanwhile waugh says his side will need to guard against complacency after convincingly winning the first test by runs waugh says despite the dominance of his side in the first test south africa can never be taken lightly it only one test match out of three or six whichever way you want to look at it so there lot of work to go he said but it nice to win the first battle definitely it gives us lot of confidence going into melbourne you know the big crowd there we love playing in front of the boxing day crowd so that will be to our advantage as well south africa begins four day match against new south wales in sydney on thursday in the lead up to the boxing day test veteran fast bowler allan donald will play in the warm up match and is likely to take his place in the team for the second test south african captain shaun pollock expects much better performance from his side in the melbourne test we still believe that we didn play to our full potential so if we can improve on our aspects the output we put out on the field will be lot better and we still believe we have side that is good enough to beat australia on our day he said»\n",
      "\n",
      "MEDIAN (80, 0.26421451568603516): «zimbabwe has been given five weeks to stop the political violence and invasions of white owned farms or face possible suspension from the commonwealth meeting in london of the commonwealth ministerial action group has listed zimbabwe the first step ahead of what could mean much tougher action under pressure from australia and the united kingdom the issue of zimbabwe consistent breach of democratic principles under the harare declaration is finally and formally on the table the commonwealth group is waiting for response from zimbabwe to request to allow observers for the upcoming election australia foreign minister alexander downer says if there is not substantial change in zimbabwe significant sanctions are possible the commonwealth ministerial action group does have number of weapons available to it and one of them is suspension he said at the same time fiji suspension was lifted after its return to democracy»\n",
      "\n",
      "LEAST (85, -0.09078729152679443): «hamas militants have fought gun battles with palestinian security forces in the gaza strip trying to arrest one of the islamic group senior political leaders reports say the fight erupted in the gaza strip after dozens of hamas members surrounded the home of abdel aziz al rantissi when palestinian police arrived to detain him the palestinian leader yasser arafat under international pressure to crack down on militants after wave of suicide bombings in israel in the past month has outlawed the military wings of hamas and other groups and arrested dozens of militants»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the most similar document (usually the same text) is has a similarity score approaching 1.0. However, the similarity score for the second-ranked documents should be significantly lower (assuming the documents are in fact different) and the reasoning becomes obvious when we examine the text itself.\n",
    "\n",
    "We can run the next cell repeatedly to see a sampling other target-document comparisons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (167): «turning grief into defiance americans have paused in remembrance three months after the deadly september attacks as resolute president george bush forecast certain victory in his war on terrorism at am new york time am aedt the exact moment when hijacked airliner steered on suicide mission sliced into one of the twin towers of the world trade centre ceremonies in washington new york and around the world honoured some people killed on an unprecedented day of horror today the wrongs are being righted justice is being done mr bush said we still have far to go and many dangers still lie ahead yet there can be no doubt how this conflict will end in new york firefighters police officers and community leaders assembled in the wreckage strewn crater where the world trade centre stood until its signature towers were levelled on the bright sunny morning of september under grey skies lone tenor sung let there be peace on earth before priest rabbi and an imam addressed solemn crowd watched by new york mayor rudy giuliani who shepherded his city through september tragedy bagpipers performed haunting rendition of amazing grace as cranes and the heavy machinery of construction workers excavating the site stood idle at the pentagon the target of another hijacked airliner us flag was unfurled in front of gaping hole in the building where reconstruction is already under way lone christmas tree daubed with red lights was fixed to the roof metres from where the plane hit defence secretary donald rumsfeld vowed never to forget the victims who died at the pentagon the thousands more killed in new york and on those on hijacked plane that crashed in rural pennsylvania apparently after the intervention of passengers in the skies over pennsylvania they showed those who believed that americans would not fight back that they were ready to roll mr rumsfeld said the first person has been charged over the terrorist attacks in the united states three months ago zaccarias moussaoui was charged in connection with the september terrorist attacks as result of the wide ranging federal probe of the attacks united states attorney general john ashcroft said moussaoui french national of moroccan origin faces six counts in connection with the terrorists attacks against the world trade centre in new york and the pentagon outside washington that left more than people dead mr ashcroft told press conference four of those counts carry the death penalty the indictment also cites list of unindicted co conspirators headed by osama bin laden head of the al qaeda network and ayman al zawahiri head of the egyptian islamic jihad as unindicted co conspirators the list of co conspirators also includes the hijackers who commandeered the four jets that were used as aerial targets on september as well as two men who sent funds to the alleged terrorists»\n",
      "\n",
      "Similar Document (140, 0.7606056928634644): «osama bin laden admitted planning the september terrorist attacks on the united states in videotape released by the pentagon today in the videotape lasting roughly one hour bin laden explains planning aspects of the operation and his own calculations in advance concerning the scale of the damage to the world trade center in new york and the number of casualties he said he expected the fire and gas from the attacks on the world trade center to topple the floors above the points where hijacked planes struck not the entire structure we calculated in advance the number of casualties from the enemy who would be killed based on the position of the tower he said according to transcript translated into english from the arabic due to my experience in this field was thinking that the fire from the gas in the plane would melt the iron structure of the building and collapse the area where the plane hit and all the floors above it only he said that is all that we had hoped for the video tape showed bin laden speaking to supporters in room possibly in kandahar in mid november the pentagon said in releasing the amateur videotape which it said was made with the knowledge of bin laden and those present the tape showed the end of the meeting first followed by an unrelated segment of videotaped material and ending with segment recorded at the beginning of the meeting we had notification since the previous thursday that the event would take place that day he said speaking to unidentified sheikh we had finished our work that day and had the radio on it was pm our time was sitting with dr ahmad abu al khair he said immediately we heard the news that plane had hit the world trade center after while they announced that another plane had hit the world trade center the brothers who heard the news were overjoyed by it he said»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same approach above, we'll infer the vector for a randomly chosen test document, and compare the document to our model by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (15): «the bush administration has drawn up plans to escalate the war of words against iraq with new campaigns to step up pressure on baghdad and rally world opinion behind the us drive to oust president saddam hussein this week the state department will begin mobilising iraqis from across north america europe and the arab world training them to appear on talk shows write opinion articles and give speeches on reasons to end president saddam rule»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d48,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (12, 0.707869291305542): «president general pervez musharraf says pakistan wants to defuse the brewing crisis with india but was prepared to respond vigorously to any attack pakistan stands for peace pakistan wants peace pakistan wants to reduce tension he said let the two countries move towards peace and harmony however pakistan has taken all counter measures if any war is thrust on pakistan the pakistan armed forces and the million people of pakistan are fully prepared to face all consequences with all their might the president said he had received the support of all political parties president musharraf also said he welcomed the intervention of the international community in trying to defuse the potentially explosive crisis we would like anybody to play useful and positive role in defusing the tension the united states the european union and the group of eight industrialised nations among others have all called on india and pakistan to exercise restraint and resolve the stand off through dialogue president musharraf repeated his offer of holding talks with indian prime minister atal behari vajpayee am for dialogue and keep on saying this and india keeps on rejecting which gives me feeling that am begging to india if they accept it we do not reject it at all he said on friday he said he was willing to meet prime minister vajpayee on the sidelines of the january south asian association for regional cooperation saarc summit in nepal india ruled out any face to face talks military tensions erupted between india and pakistan after the bloody december raid on the indian parliament india accuses pakistan military intelligence of masterminding the assault but pakistan denies the allegation with both countries massing troops along the border pakistan foreign minister abdul sattar warned saturday that the dispute was growing dangerously tense and any small act of provocation could snowball into conflict president musharraf said one of the goals of sunday meeting was to take stock of the internal situation the domestic environment want to eradicate militancy extremism intolerance from pakistani society and also said would like to eradicate any form of terrorism from the soil of pakistan however he warned the tension that has mounted on our eastern border in fact is creating obstacles and hurdles»\n",
      "\n",
      "MEDIAN (88, 0.3660702109336853): «the coroner investigating the death of race marshal at the australian formula one grand prix in melbourne has indicated he will not stand in the way of next year race on the last day of hearings into the death of race marshall graham beveridge the grand prix corporation legal team sought judicial assurance next year event would be able to go ahead coroner graeme johnston said his recommendations were unlikely to alter the corporation plans for the race and they would include nothing that could not reasonably be dealt with before the forthcoming race ross ray qc representing the grand prix corporation and the australian confederation of motor sports outlined plans to increase the height of debris fences and seek safety assessments from independent experts up to soldiers will be deployed in afghanistan later this month as an international force to provide security for the country interim government which is scheduled to take power this weekend british defence secretary geoff hoon says up to british troops will lead the force which will begin its deployment from december named the international security assistance force it will be based in the capital kabul and it is expected to be there for at least three months announcing the british deployment mr hoon said with the current tensions in afghanistan it will be difficult and sometimes dangerous mission an advance unit of british marines will be in the country by the weekend with the united nations expected to mandate the force before then australian involvement in the peacekeeping operation in afghanistan is to be determined by britain australia has offered to take part in the force but british defence sources have not revealed the nature of canberra offer the sources say britain has received too many offers for light infantry but not enough for engineering mine clearing and other logistical needs meanwhile afghan forces have begun handing over captured al qaeda fighters to new interrogation centre at the us military base of camp rhino in southern afghanistan the numbers are likely to swell as the hunt continues for al qaeda loyalists fleeing toward pakistan fifteen foreign fighters captured near the northern city of mazar sharif have now been transferred to the interrogation centre at the us marine base outside kandahar another five prisoners including the australian david hicks are being held offshore on us military ship anti taliban forces are continuing search for some al qaeda fighters believed to be still at large in the mountains around tora bora near the pakistan border local commanders said they had captured foreign fighters over the past few days and were holding them at the nearby city of jalalabad pending orders from kabul bomber circled tora bora throughout the morning but no air strikes were launched»\n",
      "\n",
      "LEAST (105, 0.027274683117866516): «fresh palls of smoke are billowing from the woomera detention centre in south australia far north trouble at the centre has entered day three with plume of smoke metres high into the air and up to metres across the compound this morning thirteen buildings were either destroyed or damaged by fire on monday night overnight fires and rioting appeared to have abated just after midnight local time three fire crews one ambulance and several police have attended the scene water cannon and three tear gas canisters were used to subdue detainees who throughout the night were thought to be chanting visa it is not known whether anyone has been injured or arrested overnight the acting immigration minister daryl williams says the government is not losing control of woomera he has told channel nine vandalism is not going to get visas for the detainees the detainees who have been provided with very good facilities and who to our knowledge have absolutely no complaint about the facilities there are engaging in this campaign of damaging and destroying buildings in order to put pressure on the australian authorities to grant them visa he said there is plea for so called high risk detainees to be separated from the rest of the population ath the woomera detention centre in the wake of continued disturbances there south australian labor mp lyn breuer whose electorate covers woomera says higher risk detainees must be separated from women and children at the centre think that will probably have to be the ultimate solution we will have to send high risk detainees to other areas she said we can keep them in an environment where there are young children there it all very nasty situation and have particular concerns for the people that are guarding them as well because one of them is going to get hurt very badly very soon»\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up\n",
    "\n",
    "That's it! Doc2Vec is a great way to explore relationships between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
